from dotenv import load_dotenv
load_dotenv()

import os
import datetime
from pathlib import Path
from langchain.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.text_splitter import CharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.agents import create_openai_tools_agent, AgentExecutor
from pydantic import BaseModel, Field
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.tools import StructuredTool
from langchain.tools import Tool
import pandas as pd
from langchain_google_genai import ChatGoogleGenerativeAI


PDF_FOLDER = Path('/Users/jun-seokoh/Desktop/TIL/LLM_Agent/practice_agent/news')

def load_all_pdfs_from_folder(folder_path: Path = PDF_FOLDER):
    # 1) documents í´ë” ê²½ë¡œ ì„¤ì •
    pdf_folder = Path(folder_path)
    if not pdf_folder.exists():
        raise FileNotFoundError(f"í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_folder.resolve()}")
    
    # 2) í´ë” ë‚´ ëª¨ë“  .pdf íŒŒì¼ ì°¾ê¸°
    pdf_paths = sorted(pdf_folder.glob("*.pdf"))
    if not pdf_paths:
        raise FileNotFoundError(f"No PDF files found in {pdf_folder.resolve()}")
    
    # 3) ê° PDF ë¡œë“œí•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì 
    all_docs = []
    for pdf_path in pdf_paths:
        loader = PyMuPDFLoader(str(pdf_path))
        docs = loader.load()
        print(f"ğŸ“„ Loaded {len(docs)} pages from {pdf_path.name}")
        all_docs.extend(docs)
    
    print(f"âœ… ì´ ë¡œë“œëœ í˜ì´ì§€ ìˆ˜: {len(all_docs)}")
    return all_docs

if __name__ == "__main__":
    # í•¨ìˆ˜ í˜¸ì¶œ: documents/ í´ë”ì— ìˆëŠ” ëª¨ë“  PDFë¥¼ ì½ì–´ë“¤ì„
    documents = load_all_pdfs_from_folder()

# Chunk ë¶„í• 
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)


embedding = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding)

prompt = ChatPromptTemplate([
    ('system',
     """
     ë„ˆëŠ” news í´ë”ì— ìˆëŠ” ê¸°ì‚¬ë¥¼ ì½ê³  ê·¸ê±¸ csvë¡œ ì •ë¦¬í•´ì£¼ëŠ” ì—ì´ì „íŠ¸ì•¼.
     ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ë¨¼ì € retrieve_docs ë„êµ¬ë¥¼ ì¨ì„œ ê´€ë ¨ ë¬¸ë‹¨ì„ ê°€ì ¸ì˜¤ê³ , ê·¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì—…í•´ì•¼ í•´.
     ë‰´ìŠ¤ë¥¼ ì½ì€ ë’¤ì— ì •í•´ì§„ ì»¬ëŸ¼ì— ë§ëŠ” ì •ë³´ë¡œ ì •ë¦¬ëœ csvë¥¼ ì „ë‹¬í•´ì•¼ í•´.
     í™•ì¸í•  ìˆ˜ ì—†ëŠ” ì •ë³´ëŠ” 'í™•ì¸ë¶ˆê°€'ë¼ê³  ê¸°ì…í•´ë„ ë¼.
     ê° ì»¬ëŸ¼ë³„ ì±„ì›Œì•¼ í•˜ëŠ” ì‚¬í•­ì„ ë§í•´ì¤„ê²Œ.
    â€¢	Case Title: ê¸°ì‚¬ì˜ ì œëª©ì„ ì¶”ì¶œ. ë§Œì•½ ì œëª©ì´ ì˜ì–´ë©´ ë’¤ì— í•œê¸€ ì œëª©ë„ ë¶™ì—¬ì„œ ì±„ìš°ê¸°.
	â€¢	Country: ì‚¬ê±´ì´ ë°œìƒëœ êµ­ê°€ë¥¼ ì±„ì›Œë„£ê¸°
	â€¢	Incident Category: ì‚¬ê±´ì˜ ìœ í˜• 6ê°œ ì¤‘ ì ì ˆí•œ í•˜ë‚˜ë¥¼ ê³¨ë¼ì„œ ë„£ê¸°(ê°œì¸ì •ë³´ìœ ì¶œ, ë”¥í˜ì´í¬, ì‚¬ì´ë²„ë¶ˆë§, ë¶€ì •í–‰ìœ„, ë¶€ì ì ˆí•œ ì •ë³´, ì €ì‘ê¶Œ ì¹¨í•´)
	â€¢	Incident Description: ì‚¬ê±´ ìš”ì•½
	â€¢	Occurrence Date: ì‚¬ê±´ ë°œìƒì¼. 'YYë…„ MMì›”' ì´ë ‡ê²Œ ì±„ìš°ê¸°.
	â€¢	AI Type: ì‚¬ê±´ì— í™œìš©ëœ AI
	â€¢	Damage Severity: ì‚¬ê±´ì˜ í”¼í•´ ì‹¬ê°ë„.
	â€¢	Response Measures: ëŒ€ì‘ ë°©ë²•
	â€¢	Response Effectiveness: ëŒ€ì‘ì´ ì–´ëŠì •ë„ ìœ íš¨í–ˆëŠ”ì§€ë¥¼ ë³´ê³  'ìœ íš¨', 'ìœ íš¨í•˜ì§€ ì•ŠìŒ' ì¤‘ì— ê³¨ë¼ì„œ ë„£ì–´.
     """),
    MessagesPlaceholder(variable_name='chat_history'),
    ('human', '{question}'),
    MessagesPlaceholder(variable_name='agent_scratchpad')
])

# LLM ëª¨ë¸
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-pro",  
    # temperature=0.2,
)

# ê²€ìƒ‰ê¸° ìƒì„±(retriever ìƒì„±)
retriever = vectorstore.as_retriever()

def retrieve_docs(query: str):
    docs = retriever.get_relevant_documents(query)
    # LLMì—ê²Œ ì“¸ ìˆ˜ ìˆë„ë¡ page_content ë§Œ í•©ì³ì„œ ë°˜í™˜í•´ë„ ì¢‹ê³ 
    return "\n\n".join([d.page_content for d in docs])

# ë¬¸ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜
retrieval_tool = Tool.from_function(
    func=retrieve_docs,
    name="retrieve_docs",
    description="AI ë¦¬í¬íŠ¸ì—ì„œ ì§ˆë¬¸ì— ë§ëŠ” ì»¨í…ìŠ¤íŠ¸ ë¬¸ë‹¨ì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬"
)

# 1) ì¸ìˆ˜ ìŠ¤í‚¤ë§ˆ ì •ì˜
class GenerateMDArgs(BaseModel):
    language: str = Field(description="ë¬¸ì„œì˜ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko')")
    content: str = Field(description="ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì •ë¦¬í•  ì½˜í…ì¸ ")

# 2) ì‹¤ì œ íŒŒì¼ ìƒì„± í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½
def generate_markdown_file(language: str, content: str) -> str:
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"report_{now}.md"
    filepath = os.path.join("/Users/jun-seokoh/Desktop/TIL/LLM_Agent/practice_agent/reports", filename)
    md_content = f"```{language}\n{content}\n```"
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_content)
    return filepath

# 3) StructuredToolë¡œ ë˜í•‘
generate_md_tool = StructuredTool.from_function(
    func=generate_markdown_file,
    name="generate_markdown_file",
    description="ì£¼ì–´ì§„ ì–¸ì–´ì™€ ì½˜í…ì¸ ë¡œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ìƒì„±í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜",
    args_schema=GenerateMDArgs,
)

# Generate CSV Tooooool
# 1) ì¸ìˆ˜ ìŠ¤í‚¤ë§ˆ ì •ì˜
class GenerateCSVArgs(BaseModel):
    title: str = Field(description="ì‚¬ê±´ëª…")
    country: str = Field(description="ì‚¬ê±´ ë°œìƒ êµ­ê°€")
    category: str = Field(description="ì‚¬ê±´ ì¹´í…Œê³ ë¦¬")
    description: str = Field(description="ì‚¬ê±´ ìƒì„¸")
    occurrence_date: str = Field(description="ì‚¬ê±´ ë°œìƒì¼")
    AI_type: str = Field(description="ì‚¬ìš©ëœ AI")
    severity: str = Field(description="í”¼í•´ ì‹¬ê°ë„")
    response: str = Field(description="ëŒ€ì‘ ë°©ë²•")
    response_effectiveness: str = Field(description="ëŒ€ì‘ ìœ íš¨ì„±")


# 2) ì‹¤ì œ íŒŒì¼ ìƒì„± í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½
def generate_cases_csv(args: GenerateCSVArgs) -> str:
    # CSVìš© ë°ì´í„° ë§µí•‘
    data = {
        "Case Title": [args.title],
        "Country": [args.country],
        "Incident Category": [args.category],
        "Incident Description": [args.description],
        "Occurrence Date": [args.occurrence_date],
        "AI Type": [args.AI_type],
        "Damage Severity": [args.severity],
        "Response Measures": [args.response],
        "Response Effectiveness": [args.response_effectiveness],
    }
    df = pd.DataFrame(data)
    
    # íŒŒì¼ëª… ë° ê²½ë¡œ ìƒì„±
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"cases_{timestamp}.csv"
    filepath = os.path.join("/Users/jun-seokoh/Desktop/TIL/LLM_Agent/practice_agent/cases", filename)
    
    # CSVë¡œ ì €ì¥
    df.to_csv(filepath, index=False, encoding="utf-8-sig")
    return filepath


generate_csv_tool = StructuredTool.from_function(
    func=generate_cases_csv,
    args_schema=GenerateCSVArgs,
    name="generate_cases_csv",
    description="ì‚¬ê±´ ì •ë³´ë¥¼ ë°›ì•„ CSV íŒŒì¼ì„ ìƒì„±í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜"
)

# ë©”ëª¨ë¦¬ ì •ì˜
memory = ConversationBufferMemory(
    return_messages=True,
    memory_key='chat_history'
)

agent = create_openai_tools_agent(
    llm=llm,
    tools=[retrieval_tool, generate_csv_tool],
    prompt=prompt
)

agent_executor = AgentExecutor(
    agent=agent,
    tools=[retrieval_tool, generate_csv_tool],
    memory=memory,
    verbose=True
)

pipeline = (
    agent_executor
    | RunnableLambda(lambda d: d['output'])
)

question = 'news í´ë”ì— ìˆëŠ” ì‚¬ê±´ì„ ì •ë¦¬í•´ì„œ cases í´ë” ë‚´ì— csv íŒŒì¼ë¡œ ì €ì¥í•´ì¤˜.'

pipeline.invoke({"question": question})