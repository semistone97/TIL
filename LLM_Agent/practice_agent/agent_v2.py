from dotenv import load_dotenv
load_dotenv()

import os
import datetime
from pathlib import Path
from langchain.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.text_splitter import CharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.agents import create_openai_tools_agent, AgentExecutor
from pydantic import BaseModel, Field
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.tools import StructuredTool
from langchain.tools import Tool

PDF_FOLDER = Path("/Users/jun-seokoh/Desktop/TIL/LLM_Agent/practice_agent/documents")

def load_all_pdfs_from_folder(folder_path: Path = PDF_FOLDER):
    # 1) documents í´ë” ê²½ë¡œ ì„¤ì •
    pdf_folder = Path(folder_path)
    if not pdf_folder.exists():
        raise FileNotFoundError(f"í´ë”ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_folder.resolve()}")
    
    # 2) í´ë” ë‚´ ëª¨ë“  .pdf íŒŒì¼ ì°¾ê¸°
    pdf_paths = sorted(pdf_folder.glob("*.pdf"))
    if not pdf_paths:
        raise FileNotFoundError(f"No PDF files found in {pdf_folder.resolve()}")
    
    # 3) ê° PDF ë¡œë“œí•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì 
    all_docs = []
    for pdf_path in pdf_paths:
        loader = PyMuPDFLoader(str(pdf_path))
        docs = loader.load()
        print(f"ðŸ“„ Loaded {len(docs)} pages from {pdf_path.name}")
        all_docs.extend(docs)
    
    print(f"âœ… ì´ ë¡œë“œëœ íŽ˜ì´ì§€ ìˆ˜: {len(all_docs)}")
    return all_docs

if __name__ == "__main__":
    # í•¨ìˆ˜ í˜¸ì¶œ: documents/ í´ë”ì— ìžˆëŠ” ëª¨ë“  PDFë¥¼ ì½ì–´ë“¤ìž„
    documents = load_all_pdfs_from_folder()

# Chunk ë¶„í• 
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)


embedding = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding)

prompt = ChatPromptTemplate([
    ('system',
     """
     ë„ˆëŠ” AI ë¦¬í¬íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì—ì´ì „íŠ¸ì•¼.
     ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ë¨¼ì € retrieve_docs ë„êµ¬ë¥¼ ì¨ì„œ ê´€ë ¨ ë¬¸ë‹¨ì„ ê°€ì ¸ì˜¤ê³ , ê·¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ë‹µí•´ì•¼ í•´.
     ë„ˆëŠ” documents í´ë” ë‚´ì— ìžˆëŠ” ë³´ê³ ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì•Œë ¤ì£¼ëŠ” AIì•¼.
     ëª¨ë“  ì‘ë‹µì€ ê¸°ë³¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´.
     ë‹¤ë§Œ ìµœì¢…ì ì¸ ì¸ì‚¬ì´íŠ¸ëŠ” í•œêµ­ì–´ë¥¼ ë§í•˜ê³  ì´í›„ì— ì¼ë³¸ì–´ë¡œ í•œ ë²ˆ ë” ë§í•˜ëŠ” ìŠµê´€ì´ ìžˆì–´.
     ê·¸ë¦¬ê³  ë„ˆëŠ” ë§ëë§ˆë‹¤ ì œ-í•˜í•˜í•˜ ë¼ëŠ” ë§ì„ ë¶™ì´ëŠ” ë²„ë¦‡ì´ ìžˆì–´.
     ê·¸ë¦¬ê³  ë„ˆëŠ” í•œ ë²ˆì”© ë‹ˆê°€ ë¡œë´‡ ë¯¸ì†Œë…€ë¥¼ ì¢‹ì•„í•œë‹¤ëŠ” ì´ìƒí•œ ì†Œë¦¬ë¥¼ í•´.
     ë„ˆëŠ” ì°¸ê³ í•œ ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…í™•ížˆ ë°í˜€ì•¼ í•´.
     """),
    MessagesPlaceholder(variable_name='chat_history'),
    ('human', '{question}'),
    MessagesPlaceholder(variable_name='agent_scratchpad')
])

# LLM ëª¨ë¸
llm = ChatOpenAI(model='gpt-4.1-nano')

# ê²€ìƒ‰ê¸° ìƒì„±(retriever ìƒì„±)
retriever = vectorstore.as_retriever()

def retrieve_docs(query: str):
    docs = retriever.get_relevant_documents(query)
    # LLMì—ê²Œ ì“¸ ìˆ˜ ìžˆë„ë¡ page_content ë§Œ í•©ì³ì„œ ë°˜í™˜í•´ë„ ì¢‹ê³ 
    return "\n\n".join([d.page_content for d in docs])

# ë¬¸ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜
retrieval_tool = Tool.from_function(
    func=retrieve_docs,
    name="retrieve_docs",
    description="AI ë¦¬í¬íŠ¸ì—ì„œ ì§ˆë¬¸ì— ë§žëŠ” ì»¨í…ìŠ¤íŠ¸ ë¬¸ë‹¨ì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬"
)

# 1) ì¸ìˆ˜ ìŠ¤í‚¤ë§ˆ ì •ì˜
class GenerateMDArgs(BaseModel):
    language: str = Field(description="ë¬¸ì„œì˜ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko')")
    content: str = Field(description="ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì •ë¦¬í•  ì½˜í…ì¸ ")

# 2) ì‹¤ì œ íŒŒì¼ ìƒì„± í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½
def generate_markdown_file(language: str, content: str) -> str:
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"report_{now}.md"
    filepath = os.path.join("/Users/jun-seokoh/Desktop/TIL/LLM_Agent/practice_agent/reports", filename)
    md_content = f"```{language}\n{content}\n```"
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_content)
    return filepath

# 3) StructuredToolë¡œ ëž˜í•‘
generate_md_tool = StructuredTool.from_function(
    func=generate_markdown_file,
    name="generate_markdown_file",
    description="ì£¼ì–´ì§„ ì–¸ì–´ì™€ ì½˜í…ì¸ ë¡œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ìƒì„±í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜",
    args_schema=GenerateMDArgs,
)

# ë©”ëª¨ë¦¬ ì •ì˜
memory = ConversationBufferMemory(
    return_messages=True,
    memory_key='chat_history'
)

agent = create_openai_tools_agent(
    llm=llm,
    tools=[retrieval_tool, generate_md_tool],
    prompt=prompt
)

agent_executor = AgentExecutor(
    agent=agent,
    tools=[retrieval_tool, generate_md_tool],
    memory=memory,
    verbose=True
)

pipeline = (
    agent_executor
    | RunnableLambda(lambda d: d['output'])
)
print('ë¦¬í¬íŠ¸ í•™ìŠµ Agentì™€ ëŒ€í™” ì‹œìž‘(endë¡œ ì¢…ë£Œ)')
while True:
    question = input('>>> ')
    if question.lower() == 'end':
        break
    print('Q:', question)
    pipeline.invoke({"question": question})